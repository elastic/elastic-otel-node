# Update this with your real OpenAI API key
OPENAI_API_KEY=sk-YOUR_API_KEY

# Uncomment to use Ollama instead of OpenAI
# OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_API_KEY=unused
# CHAT_MODEL=qwen2.5:0.5b
# EMBEDDINGS_MODEL=all-minilm:33m

# OTEL_EXPORTER_* variables are not required. If you would like to change your
# OTLP endpoint to Elastic APM server using HTTP, uncomment the following:
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:8200
# OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf

OTEL_SERVICE_NAME=openai-example
OTEL_LOG_LEVEL=warn

# Change to 'false' to hide prompt and completion content
OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=true
# Change to affect behavior of which resources are detected. Note: these
# choices are specific to the runtime, in this case Node.js.
OTEL_NODE_RESOURCE_DETECTORS=container,env,host,os,serviceinstance,process,alibaba,aws,azure
